{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook implementing a Danish BERT in order to predict named entities in invoicedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "source": [
    "import os\n",
    "os.chdir(\"/bachelor_project\")\n",
    "\n",
    "MODEL_DIR = \"/bachelor_project/danish_bert_uncased_v2\"\n",
    "\n",
    "!transformers-cli convert --model_type bert \\\n",
    "  --tf_checkpoint $MODEL_DIR/bert_model.ckpt \\\n",
    "  --config $MODEL_DIR/bert_config.json \\\n",
    "  --pytorch_dump_output $MODEL_DIR/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading packages\n",
    "## Standard packages\n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## pyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "## Transformers\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "## Other ML utils\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from tqdm import tqdm,trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/bachelor_project\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and inspecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/KMD/NER_data_split/NERtrain.csv\", encoding = \"UTF-8\")\n",
    "train['countcolumn'] = 'Sentence: ' + train['countcolumn'].astype(str) #adding 'Sentence:' to the 'Sentence #' column\n",
    "#df = df.drop(\"InvoiceNo\", axis = 1)\n",
    "#df = df[[\"SentenceNo\", \"word\", \"labels\"]]\n",
    "train=train.rename(columns={\"countcolumn\": \"SentenceNo\"})\n",
    "\n",
    "val = pd.read_csv(\"data/KMD/NER_data_split/NERval.csv\", encoding = \"UTF-8\")\n",
    "val['countcolumn'] = 'Sentence: ' + val['countcolumn'].astype(str) #adding 'Sentence:' to the 'Sentence #' column\n",
    "#df = df.drop(\"InvoiceNo\", axis = 1)\n",
    "#df = df[[\"SentenceNo\", \"word\", \"labels\"]]\n",
    "val=val.rename(columns={\"countcolumn\": \"SentenceNo\"})\n",
    "\n",
    "test = pd.read_csv(\"data/KMD/NER_data_split/NERtest.csv\", encoding = \"UTF-8\")\n",
    "test['countcolumn'] = 'Sentence: ' + test['countcolumn'].astype(str) #adding 'Sentence:' to the 'Sentence #' column\n",
    "#df = df.drop(\"InvoiceNo\", axis = 1)\n",
    "#df = df[[\"SentenceNo\", \"word\", \"labels\"]]\n",
    "test=test.rename(columns={\"countcolumn\": \"SentenceNo\"})\n",
    "\n",
    "#Example of training set\n",
    "train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Sanity checking number of words\n",
    "print(len(train.labels) == 121865)\n",
    "print(len(val.labels) == 6792)\n",
    "print(len(test.labels) == 7416)\n",
    "\n",
    "print(len(train.labels))\n",
    "print(len(val.labels))\n",
    "print(len(test.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Sanity checking sentences, counting unique words and counting number of labels\n",
    "print(train.SentenceNo.nunique() == 30733, train.word.nunique() == 26773, train.labels.nunique() == 5)\n",
    "print(val.SentenceNo.nunique() == 1707, val.word.nunique() == 3350, val.labels.nunique() == 5)\n",
    "print(test.SentenceNo.nunique() == 1707, test.word.nunique() == 2633, test.labels.nunique() == 5)\n",
    "\n",
    "print(train.SentenceNo.nunique(), train.word.nunique(), train.labels.nunique())\n",
    "print(val.SentenceNo.nunique(), val.word.nunique(), val.labels.nunique())\n",
    "print(test.SentenceNo.nunique(), test.word.nunique(), test.labels.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity checking\n",
    "train.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity checking\n",
    "val.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity checking\n",
    "train.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the sentences from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function that extracts each sentence and its labels from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(word, label) for word, label in zip(s[\"word\"].values.tolist(),\n",
    "                                                     s[\"labels\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"SentenceNo\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating variable \"sentences\" that lists all sentences from the dataframe and the variable \"labels\" that lists all the corresponding labels for each word in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full document data structure\n",
    "tr_sentences = []\n",
    "tr_labels = []\n",
    "getter = SentenceGetter(train)\n",
    "# Get sentence data\n",
    "tr_sentences = [[s[0] for s in sent] for sent in getter.sentences]\n",
    "# Get tag labels data\n",
    "tr_labels = [[s[1] for s in sent] for sent in getter.sentences]\n",
    "\n",
    "# Get full document data structure\n",
    "val_sentences = []\n",
    "val_labels = []\n",
    "getter = SentenceGetter(val)\n",
    "# Get sentence data\n",
    "val_sentences = [[s[0] for s in sent] for sent in getter.sentences]\n",
    "# Get tag labels data\n",
    "val_labels = [[s[1] for s in sent] for sent in getter.sentences]\n",
    "\n",
    "# Get full document data structure\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "getter = SentenceGetter(test)\n",
    "# Get sentence data\n",
    "test_sentences = [[s[0] for s in sent] for sent in getter.sentences]\n",
    "# Get tag labels data\n",
    "test_labels = [[s[1] for s in sent] for sent in getter.sentences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT model requires input data to be in a specific format. One requirement is to have special tokens that marks the beginning ([CLS]) and the separation/end of sentences ([SEP]). These tokens are added to the list of label values below. Furthermore, the label [PAD] is added to indicate padded tokens after padding the sentences later in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding labels to fine-tune the BERT\n",
    "tag_values = list(train.labels.unique())\n",
    "tag_values.append(\"[PAD]\")\n",
    "tag_values.append(\"[CLS]\")\n",
    "tag_values.append(\"[SEP]\")\n",
    "print(tag_values)\n",
    "\n",
    "#Creating tag to index and index to tags variables\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "print(tag2idx)\n",
    "print(idx2tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the training data using the vocabulary from danish BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT provides its own tokenizer which is imported below. The tokenizer is created with a Wordpiece model and it creates a vocabulary of whole words, subwords and individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load tokenizer, with manual file address or pretrained address from the Transformers library\n",
    "tokenizer = BertTokenizer.from_pretrained(\"danish_bert_uncased_v2/vocab.txt\", do_lower_case = True, strip_accents = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(s, l)\n",
    "    for s, l in zip(tr_sentences, tr_labels)\n",
    "]\n",
    "\n",
    "val_tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(s, l)\n",
    "    for s, l in zip(val_sentences, val_labels)\n",
    "]\n",
    "\n",
    "test_tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(s, l)\n",
    "    for s, l in zip(test_sentences, test_labels)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_tokenized_texts_and_labels[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tr_tokenized_texts = [[\"[CLS]\"] + tr_token_label_pair[0] + [\"[SEP]\"] for tr_token_label_pair in tr_tokenized_texts_and_labels]\n",
    "tr_labels = [[\"[CLS]\"] + tr_token_label_pair[1] + [\"[SEP]\"] for tr_token_label_pair in tr_tokenized_texts_and_labels]\n",
    "\n",
    "\n",
    "val_tokenized_texts = [[\"[CLS]\"] + val_token_label_pair[0] + [\"[SEP]\"] for val_token_label_pair in val_tokenized_texts_and_labels]\n",
    "val_labels = [[\"[CLS]\"] + val_token_label_pair[1] + [\"[SEP]\"] for val_token_label_pair in val_tokenized_texts_and_labels]\n",
    " \n",
    "\n",
    "test_tokenized_texts = [[\"[CLS]\"] + test_token_label_pair[0] + [\"[SEP]\"] for test_token_label_pair in test_tokenized_texts_and_labels]\n",
    "test_labels = [[\"[CLS]\"] + test_token_label_pair[1] + [\"[SEP]\"] for test_token_label_pair in test_tokenized_texts_and_labels]\n",
    "\n",
    "#Example of word-piece tokenizations:\n",
    "print(tr_tokenized_texts[10])\n",
    "print(tr_labels[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that number of words in all datasets are increased due to the word-piece tokenization. For the test dataset this means that it will have a higher number of words i.e. labels also during evaluation and comparison to the rule-based classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of words increased from 7,416 to 22,424\n",
    "tmp=0\n",
    "for labels in test_labels:\n",
    "    tmp=tmp+len(labels)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Len of the sentence must be not bigger than the training model\n",
    "# See model's 'max_position_embeddings' = 512\n",
    "\n",
    "MAX_LEN = len(max(tr_tokenized_texts, key = len))\n",
    "print(MAX_LEN)\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing tokens in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tr_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tr_tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "\n",
    "val_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in val_tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "test_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in test_tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "#Example of indexing\n",
    "print(tr_input_ids[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tr_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in tr_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"[PAD]\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "\n",
    "val_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in val_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"[PAD]\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "\n",
    "test_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in test_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"[PAD]\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "#Example of indexing\n",
    "print(tr_tags[10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating attention masks that indicates which elements in the sentence are tokens and which are padding elements. So here we create the mask to ignore the padded elements in the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tr_attention_masks = [[float(i != 0) for i in ii] for ii in tr_input_ids]\n",
    "\n",
    "\n",
    "val_attention_masks = [[float(i != 0) for i in ii] for ii in val_input_ids]\n",
    "\n",
    "test_attention_masks = [[float(i != 0) for i in ii] for ii in test_input_ids]\n",
    "\n",
    "#Example of attention masks\n",
    "print(tr_attention_masks[10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch requires converting datasets into torch tensors (multidimensional matrices). Inputs, tags and mask ID's for training and test data are converted to tensors and moved to the GPU by applying .to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_input_ids)\n",
    "val_inputs = torch.tensor(val_input_ids)\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "test_tags = torch.tensor(test_tags)\n",
    "\n",
    "tr_masks = torch.tensor(tr_attention_masks)\n",
    "val_masks = torch.tensor(val_attention_masks)\n",
    "test_masks = torch.tensor(test_attention_masks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating training and test tensor datasets and defining data loaders. Shuffling the training data with RandomSampler and at test time we just pass them sequentially with the SequentialSampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_tags)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the pre-trained bert-base-cased model and provide the number of possible labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pt_model_dir = \"/bachelor_project/danish_bert_uncased_v2/\"\n",
    "\n",
    "# Will load config and weight with from_pretrained(). \n",
    "model = BertForTokenClassification.from_pretrained(pt_model_dir,num_labels=len(tag2idx), output_attentions = False, output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to GPU,if you are using GPU machine\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of trainable parameters: {model.num_parameters()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting full finetuning to true because we have capacity to fine tune all layers / update all weights. Before we can start the fine-tuning process, we have to setup the optimizer and add the parameters it should update. A common choice is the AdamW optimizer. We also add some weight_decay as regularization to the main weight matrices. If you have limited resources, you can also try to just train the linear classifier on top of BERT and keep all other weights fixed. This will still give you a good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_tok = tokenizer.vocab[\"[PAD]\"]\n",
    "sep_tok = tokenizer.vocab[\"[SEP]\"]\n",
    "cls_tok = tokenizer.vocab[\"[CLS]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(valid_tags, pred_tags):\n",
    "\n",
    "    \"\"\"\n",
    "    Define a flat accuracy metric to use while training the model.\n",
    "    \"\"\"\n",
    "\n",
    "    return (np.array(valid_tags) == np.array(pred_tags)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot_confusion_matrix(valid_tags, pred_tags):\n",
    "\n",
    "    \"\"\"\n",
    "    Create an annotated confusion matrix by adding label\n",
    "    annotations and formatting to sklearn's `confusion_matrix`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create header from unique tags\n",
    "    header = sorted(list(set(valid_tags + pred_tags)))\n",
    "\n",
    "    # Calculate the actual confusion matrix\n",
    "    matrix = confusion_matrix(valid_tags, pred_tags, labels=['B-LOC', 'B-PER', 'I-LOC', 'I-PER'])\n",
    "\n",
    "    # Final formatting touches for the string output\n",
    "    mat_formatted = [header[i] + \"\\t\" + str(row) for i, row in enumerate(matrix)]\n",
    "    content = \"\\t\" + \" \".join(header) + \"\\n\" + \"\\n\".join(mat_formatted)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,        \n",
    "    lr=3e-5 #The authors of BERT uses 3e-5 as lr for BERT-base\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4 # Train a maximum of 3-4 epochs. More will simply result in overfitting the training data. \n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "tr_loss_values, eval_loss_values = [], []\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    epoch += 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"\\nStarting training loop.\")\n",
    "    model.train()\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Add batch to gpu\n",
    "        batch = tuple(t.to(torch.int64).to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels,\n",
    "        )\n",
    "        loss, tr_logits = outputs[:2]\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Compute train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        # Subset out unwanted predictions on CLS/PAD/SEP tokens\n",
    "        preds_mask = (\n",
    "            (b_input_ids != cls_tok)\n",
    "            & (b_input_ids != pad_tok)\n",
    "            & (b_input_ids != sep_tok)\n",
    "        )\n",
    "\n",
    "        #preds_mask = preds_mask.detach().cpu().numpy()\n",
    "        tr_logits = tr_logits.detach().cpu().numpy()\n",
    "        tr_label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
    "        tr_batch_preds = np.argmax(tr_logits[preds_mask.detach().cpu().numpy().squeeze()], axis=1)\n",
    "        tr_batch_labels = tr_label_ids.to(\"cpu\").numpy()\n",
    "        tr_preds.extend(tr_batch_preds)\n",
    "        tr_labels.extend(tr_batch_labels)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        tmp_tr_accuracy = flat_accuracy(tr_batch_labels, tr_batch_preds)\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=max_grad_norm\n",
    "        )\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "    tr_loss = tr_loss / nb_tr_steps\n",
    "    tr_loss_values.append(tr_loss)\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "\n",
    "    # Print training loss and accuracy per epoch\n",
    "    print(f\"Train loss: {tr_loss}\")\n",
    "    print(f\"Train accuracy: {tr_accuracy}\")\n",
    "\n",
    "    # Validation loop\n",
    "    print(\"Starting validation loop.\")\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    for batch in valid_dataloader:\n",
    "\n",
    "        batch = tuple(t.to(torch.int64).to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "        # Subset out unwanted predictions on CLS/PAD/SEP tokens\n",
    "        preds_mask = (\n",
    "            (b_input_ids != cls_tok)\n",
    "            & (b_input_ids != pad_tok)\n",
    "            & (b_input_ids != sep_tok)\n",
    "        )\n",
    "\n",
    "        logits = logits.to(\"cpu\").numpy()\n",
    "        label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
    "        val_batch_preds = np.argmax(logits[preds_mask.detach().cpu().numpy().squeeze()], axis=1)\n",
    "        val_batch_labels = label_ids.to(\"cpu\").numpy()\n",
    "        predictions.extend(val_batch_preds)\n",
    "        true_labels.extend(val_batch_labels)\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(val_batch_labels, val_batch_preds)\n",
    "\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "\n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Evaluate loss, acc, conf. matrix, and class. report on validation set\n",
    "    pred_tags = [idx2tag[i] for i in predictions]\n",
    "    valid_tags = [idx2tag[i] for i in true_labels]\n",
    "    cl_report = classification_report(valid_tags, pred_tags, labels=['B-LOC', 'B-PER', 'I-LOC', 'I-PER'])\n",
    "    conf_mat = annot_confusion_matrix(valid_tags, pred_tags)\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_loss_values.append(eval_loss)\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    f1score = f1_score(valid_tags, pred_tags, labels = ['B-LOC', 'B-PER', 'I-LOC', 'I-PER'], average=\"micro\")\n",
    "\n",
    "    # Report metrics\n",
    "    print(f\"Validation loss: {eval_loss}\\n\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\\n\")\n",
    "    print(f\"F1-Score: {f1score}\\n\")\n",
    "    print(f\"Classification Report:\\n {cl_report}\")\n",
    "    print(f\"Confusion Matrix:\\n {conf_mat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(tr_loss_values, 'b-o', label=\"training loss\")\n",
    "plt.plot(eval_loss_values, 'r-o', label=\"validation loss\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model_path = 'research/KMD/NER/models/danish_BERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dir if not exits\n",
    "if not os.path.exists(ner_model_path):\n",
    "        os.makedirs(ner_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model and the tokenizer\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model #Take care of distrubuted/parallel training\n",
    "model_to_save.save_pretrained(ner_model_path)\n",
    "tokenizer.save_pretrained(ner_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model to tokenize the test sentece\n",
    "model = BertForTokenClassification.from_pretrained(ner_model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(ner_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pad_tok = tokenizer.vocab[\"[PAD]\"]\n",
    "sep_tok = tokenizer.vocab[\"[SEP]\"]\n",
    "cls_tok = tokenizer.vocab[\"[CLS]\"]\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "predictions, true_labels = [], []\n",
    "tr_loss_values, test_loss_values = [], []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "\n",
    "    batch = tuple(t.to(torch.int64).to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels,\n",
    "        )\n",
    "        tmp_test_loss, logits = outputs[:2]\n",
    "\n",
    "    # Subset out unwanted predictions on CLS/PAD/SEP tokens\n",
    "    preds_mask = (\n",
    "        (b_input_ids != cls_tok)\n",
    "        & (b_input_ids != pad_tok)\n",
    "        & (b_input_ids != sep_tok)\n",
    "    )\n",
    "\n",
    "    logits = logits.to(\"cpu\").numpy()\n",
    "    label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
    "    test_batch_preds = np.argmax(logits[preds_mask.detach().cpu().numpy().squeeze()], axis=1)\n",
    "    test_batch_labels = label_ids.to(\"cpu\").numpy()\n",
    "    predictions.extend(test_batch_preds)\n",
    "    true_labels.extend(test_batch_labels)\n",
    "\n",
    "    tmp_test_accuracy = flat_accuracy(test_batch_labels, test_batch_preds)\n",
    "\n",
    "    test_loss += tmp_test_loss.mean().item()\n",
    "    test_accuracy += tmp_test_accuracy\n",
    "\n",
    "\n",
    "    nb_test_examples += b_input_ids.size(0)\n",
    "    nb_test_steps += 1\n",
    "\n",
    "# Evaluate loss, acc, conf. matrix, and class. report on devset\n",
    "pred_tags = [idx2tag[i] for i in predictions]\n",
    "valid_tags = [idx2tag[i] for i in true_labels]\n",
    "cl_report = classification_report(valid_tags, pred_tags, labels=['B-LOC', 'B-PER', 'I-LOC', 'I-PER'])\n",
    "conf_mat = annot_confusion_matrix(valid_tags, pred_tags)\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_loss_values.append(test_loss)\n",
    "test_accuracy = test_accuracy / nb_test_steps\n",
    "f1score_micro = f1_score(valid_tags, pred_tags, labels = ['B-LOC', 'B-PER', 'I-LOC', 'I-PER'], average=\"micro\")\n",
    "f1score_macro = f1_score(valid_tags, pred_tags, labels = ['B-LOC', 'B-PER', 'I-LOC', 'I-PER'], average=\"macro\")\n",
    "\n",
    "# Report metrics\n",
    "print(f\"Number of Epochs: {epochs}\\n\")\n",
    "\n",
    "print(f\"Test loss: {test_loss}\\n\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\\n\")\n",
    "\n",
    "print(f\"F1-Score Micro: {f1score_micro}\\n\")\n",
    "print(f\"F1-Score Macro: {f1score_macro}\\n\")\n",
    "\n",
    "\n",
    "print(f\"Classification Report:\\n {cl_report}\")\n",
    "print(f\"Confusion Matrix:\\n {conf_mat}\")\n",
    "\n",
    "with open(f'{ner_model_path}/TESTMETRICS','a+') as f:\n",
    "    f.write(f\"Number of Epochs: {epochs}\\n\")\n",
    "\n",
    "    f.write(f\"Test loss: {test_loss}\\n\")\n",
    "    f.write(f\"Test Accuracy: {test_accuracy}\\n\")\n",
    "\n",
    "    f.write(f\"F1-Score Micro: {f1score_micro}\\n\")\n",
    "    f.write(f\"F1-Score Macro: {f1score_macro}\\n\")\n",
    "\n",
    "    f.write(f\"Classification Report:\\n {cl_report}\")\n",
    "    f.write(f\"Confusion Matrix:\\n {conf_mat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model's capabilities on specific tokens only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test[\"labels\"][test[\"labels\"].str.contains(\"PER\")]=\"B-PER\"\n",
    "valid_tags = pd.Series(valid_tags)\n",
    "valid_tags[valid_tags.str.contains(\"PER\")] = \"B-PER\"\n",
    "valid_tags[valid_tags.str.contains(\"LOC\")] = \"B-LOC\"\n",
    "valid_tags = valid_tags.tolist()\n",
    "\n",
    "pred_tags = pd.Series(pred_tags)\n",
    "pred_tags[pred_tags.str.contains(\"PER\")] = \"B-PER\"\n",
    "pred_tags[pred_tags.str.contains(\"LOC\")] = \"B-LOC\"\n",
    "pred_tags = pred_tags.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cl_report = classification_report(valid_tags, pred_tags, labels = ['B-LOC', 'B-PER'])\n",
    "conf_mat = confusion_matrix(valid_tags, pred_tags)\n",
    "f1score = f1_score(valid_tags, pred_tags, labels = ['B-LOC', 'B-PER'], average = \"micro\")\n",
    "\n",
    "# Report metrics\n",
    "print(f\"F1-Score: {f1score}\\n\")\n",
    "print(f\"Classification Report:\\n {cl_report}\")\n",
    "print(f\"Confusion Matrix:\\n {conf_mat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Indkøb af Melon 1 kg, 2 slags Karen Volf 200g, 2 poser Chili og Timian fra Santa Maria, Arla 1L. Vores referencer: Karen Volf, Chili Jensen, Timian Hansen og Arla Kristoffersen. Kontaktperson er Melon Andersen. Levering til Timianvej 12\"\n",
    "test_sentence2 = \"Timian Nielsen har bestilt 10 kasser Lego til levering på Hc. Andersensvej 13 A første Sal tv og han har købt det til sin datter chili som går med Åben ble fra Abena og hun elsker i øvrigt elsker at spise chili, så derfor har de 10 kg chili derhjemme, men hvad chili ikke ved er at hendes far har købt en hvid 3 hjulet cykel fra Toys R Us ved Toppen Nr. 3 Aarhus- helt specifikt er det en 3 hjulet nr 30 fra kataloget og han har husket Toppen beskyttelseshjelm, str 35 og Far Timian kan godt lide chiLi men han elsker at spise en Tivoli stang, derfor bestilte han 20 stk Toms tivoli stang så han kan dele med sin ven Sebsatian i stedet for at få Melon i Grøn Box.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([tokenized_sentence]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(input_ids)\n",
    "logits = F.softmax(logits[0], dim = 2)\n",
    "logits_label = torch.argmax(logits, dim = 2)\n",
    "logits_label = logits_label.detach().cpu().numpy().tolist()[0]\n",
    "\n",
    "logits_confidence = [values[label].item() for values, label in zip(logits[0], logits_label)]\n",
    "len(logits_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join bpe split tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "new_tokens, new_labels, new_probs = [], [], []\n",
    "for token, label_idx, probs in zip(tokens, logits_label, logits_confidence):\n",
    "    if token.startswith(\"##\"):\n",
    "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "    else:\n",
    "        new_labels.append(tag_values[label_idx])\n",
    "        new_tokens.append(token)\n",
    "        new_probs.append(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for token, label, prob in zip(new_tokens, new_labels, new_probs):\n",
    "    print(\"{}\\t{}\\t{}\".format(label, token, prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_predictions = [{\"Word\":token,\"Label\":label,\"Confidence\":prob} for token, label, prob in zip(new_tokens, new_labels, new_probs)]\n",
    "dict_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   },
   "name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
